# 话题检测新课

EM Compare: E-step, M-step, Log-Likelihood, 看例子

混合模型行为分析



# 1 Intro

多数是了解；

机器学习算法的组件化思想，第23页：

5个组件及各自意思，实践中怎么拆成5件；

3个核心组件；

给定一个算法能够拆出来。

理解，不背。

具体每个组件的介绍、模型模式、关系、评分函数；

不用背常用算法；

表格，不背，理解。

# 2 频繁模式

## a 概念+Apriori

basic concepts：11页，支持度、可信度的概念，在例子中理解、会算（13页）；最小支持度用在找频繁项集，最小可信度用于找关联规则（给定）。

算法，Apriori和FP-Growth

Apriori重点是两步，产生频繁项集和关联规则，理解例子30页，伪代码；规则的可信度，是不是关联规则。必考，送分，及格。

## b 序列

序列。怎么求子序列，13页。序列支持度，是不是频繁序列

GSP算法，和Apriori类似；关联规则和序列模式生成候选集时的区别（序列关注顺序）；对itemset计数的区别。

序列规则，54页，无序、非空、不相交



序列支持度，序列可信度

不要求序列规则的算法求，给规则能判断是否是规则

应用实例理解即可

## c 图

不考

## d FP-Growth

也很重要；比Apriori难理解

深度优先；要求能理解，会算

（第9页，给一个数据库，能去重、找到频繁项集后整理为数据库，生成flist，<u>构造fptree</u>）

（第10页，怎么找到所有条件模式基，不会手动求到尾，给fp树，告知在哪里找条件模式基，例如求m的条件模式基，和m一起出现的fca:2和fcab:1）（要么是给状态求树，要么是给树求某个的条件模式基，还要能画出树（给数据库/给条件模式基（同样看成表，求频繁项集）（深度优先到m分支后的继续求树）（以及求m合并到a、m合并到b（ma、mc），ma、mc、mf的条件模式基））（给到中间状态会求））

9、10、11、12、13、14、15

后面的了解即可。

## e

扩展性的内容；

相关分析时，求相关性

# 3 有监督学习

## a

前面的概念，理解

理解维度灾难（11页，为什么单位体积变小；变小就需要更多样本）

（中间的，大家应该都会）

决策树：怎么求熵，信息是怎么回事

树根？：3步，不用背，要算会告诉公式；37页；主要是理解，38页的数据集；41，计算左右子树该选择哪个属性作为树根

模型评估和选择：58，59页；四类数据各用来干嘛；60页

63页，了解；

65页，数据集划分方法：留出，交叉验证，留一法，自助法；大概思路，理解如何划分，会用方法，分出来长什么样子。

给一个数据集，划分。

73，评估指标，混淆矩阵，知道是什么，怎么表达，写出来；二元分类混淆矩阵，算74、75、76几个指标。

指标间的关系，用于进一步理解；

f1度量，怎么算。

ROC曲线，理解横纵坐标，怎么度量的；对于一个roc曲线，位于哪里的模型最理想，82

AUC，是什么，越大越好

知道AUC、ROC，什么情况好什么情况不好

了解，不均衡，多元分类等

方差偏差，不要求数学公式、计算；理解方差和偏差代表什么，88页；什么情况下方差大，偏差大，两者的公式。在什么地方，两者平衡，最恰当的，复杂度适当，适度拟合，92页；具象化理解。

## b

SVM，相对复杂，数学表达与推导到核技巧不要求。推导前，最大边缘是什么，其优化目标是什么；支持向量机模型符号是什么意思（f(x,w,b)=sign(w·x+b)，正在线上方，负在线下方，虚圈，二分类问题；理解）。

线性不可分中，核函数；数学公式不理解可以，作为铺垫、原始问题转换为了；预测函数，映射函数的内积；核方法求内积，数学家能在不知道\phi的时候怎么求内积；常用和函数；64页，数学家展开。

内积可以通过在原始空间中计算K(x,y)=...得出。常用核函数，66页；线性核函数等于没用；核函数理解到64页即可。不需要知道\phi是什么也能算，避免计算代价。

这个ppt是优秀率所用。一旦出就难

## c

神经网络；片段拿出来，不会手算反向传播

神经元，线性加和+激活函数（最简单的就是阈值）（加了个w0，以及其他激活函数（了解））

神经网络的历史，分类器，后向传播，理解即可；68页，例子，理解，如果要算，公式会给；一般重要，一般不会要求算

卷积神经网络，卷积操作（怎么算，给一个矩阵，……）（送分）。

padding，stride，求卷积结果大小时需要知道概念。

105的例子；110的公式理解推导，卷积完后输出图的大小，非常重要

加个padding后怎么算，padding为1的话……（加一圈，N+2；padding with2，N+4）

117计算。5x5大小的卷积核，……算参数大小，输出图大小（用于验证结果对不对

pooling layer，池化方法，最大和平均（送分）

128、129例子。

图内容不考。

后向传播算法和求导，不会要求算；求导公式也不多，63页，理解即可。

3种梯度下降的方法；批量梯度下降，随机，小批量

激活函数，理解

# 4 

## a KM

曼哈顿距离和欧几里得距离，什么意思怎么算

KMeans，过程（3步走）；17页的1维例子，给出两次迭代的结果等。

二维情况下迭代两次后的结果（21页，聚类结果等，算；换成欧几里得函数，等）

送分

## b dbscan

宽度优先

dbscan，5个概念

optics，2个概念



理解，不会背概念；给一个图，哪个是密度可达，等等

18页，超参数，体会超参数对dbscan的影响；哪一种情况下模型最优



optics，核心距离，可达距离；理解，才能用方法，把图画出来；算法过程理解即可。

3种方法，km、dbscan、optics的联系；19页的优缺点，优点对照km（km，球状簇，先知道簇个数，对噪声敏感），缺点（2个参数，链条现象（引入cure，但没讲），对密度不均的数据集性能下降，故引入optics；最后一点不考虑）。（常识，重要）

## cd

高斯混合模型GMM

公式多，很难说具体考什么；

GMM和话题检测的内容，最终揉到一起，会EM的例子即可（p()用的一元文法模型）

GMM能理解多少理解多少

第8页，每一个模型自己出现的概率+……在前景词出现的概率+在后景词出现的概率，是GMM k=2的简版，全概率展开公式



第15页，给某一个词，是前景还是后景词，P(w\in\theta_d),P(w\in\theta_B)

似然函数，所有的对象出现概率乘积（13，对数似然，log后就是+）



3个公式，8，11，15（Px，似然，每个对象属于某个class）；落实到话题检测里的题目，给文本，次数知道，比例知道，初始值知道，算下去



不会有问答题，会有计算题，理解后能做出来的题。卷积、聚类的结果，等

期末就是上面讲的原题修改
